[<img src="https://ko-fi.com/img/githubbutton_sm.svg">](https://ko-fi.com/lagkitty)

# ğŸ¤– Ollama Terminal

An autonomous terminal agent powered by a **locally running Ollama model**.

Describe a task, it plans the steps, runs real shell commands, reads the output, fixes errors, and continues until the task is fully complete.

âœ… 100% local  
âœ… No API keys  
âœ… No cloud  
âœ… No data leaves your machine  

<sub>If you donâ€™t trust this `.py` file, you can upload it to any AI or review tool and verify that itâ€™s safe.</sub>

---

## ğŸ–¥ Full Demo

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ğŸ¤–   O L L A M A   T E R M I N A L   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â— Ollama: running   2 model(s)

Main Menu
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. â–¶ Run Task              auto-select model
2. âš™ Run Task              choose model manually
3. â­ Saved Tasks
4. â†“ Pull a Model
5. âœ“ System Check
6. âœ Custom Instructions
7. âŸ³ Start Ollama
0. Exit
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Choice: 1

Auto-selected: llama3:latest

What do you want me to do?
> set up a python project called weather-cli with a venv, install requests, and write a hello world script

Connected (chat)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model: llama3:latest
Task:  set up a python project called weather-cli with a venv, install requests, and write a hello world script
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1  explore home directory
â”Œâ”€ $ ls /home/alex
â”‚ Desktop  Documents  Downloads  Music  Pictures
â””â”€ âœ“

Step 2  create project folder
â”Œâ”€ $ mkdir -p /home/alex/Documents/weather-cli
â””â”€ âœ“

Step 3  create virtual environment
â”Œâ”€ $ python3 -m venv /home/alex/Documents/weather-cli/venv
â””â”€ âœ“

Step 4  install requests into venv
â”Œâ”€ $ /home/alex/Documents/weather-cli/venv/bin/pip install requests
â”‚ Successfully installed requests
â””â”€ âœ“

Step 5  write hello world script

â“ Agent asks: Should the script print to stdout or write to a log file?
   Your answer: stdout is fine

â”Œâ”€ $ printf 'print("Hello from weather-cli!")\n' > /home/alex/Documents/weather-cli/main.py
â””â”€ âœ“

Step 6  verify script runs correctly
â”Œâ”€ $ /home/alex/Documents/weather-cli/venv/bin/python main.py
â”‚ Hello from weather-cli!
â””â”€ âœ“

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ“  Task complete!

Created weather-cli project with venv, installed requests,
wrote and verified main.py â€” all in /home/alex/Documents/weather-cli
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Save as a saved task? [y/N]: y
âœ“ Saved!
```

(Alex is just an example name.)

---

## ğŸš€ What It Can Do

- Create projects  
- Install packages  
- Manage files  
- Debug errors  
- Run system maintenance  
- Use web search when needed  
- Break large tasks into safe, small steps  

Everything runs in your real shell.

---

## âœ¨ Core Features

- Autonomous execution loop  
- Live streaming command output  
- Self-correcting error handling  
- Strict JSON action format  
- Automatically detects:
  - Username  
  - Home directory  
  - Operating system  
  - Shell  
  - Current working directory  
  - Available package managers  
- Saved tasks  
- Custom persistent instructions  
- Auto-starts Ollama if needed  
- Works with most Ollama models  

---

## ğŸ“¦ Requirements

- Python 3.8+  
- Ollama installed  
- `requests` library  

Install dependency:

```bash
pip install requests
```

Pull at least one model:

```bash
ollama pull llama3
```

---

## â–¶ Usage

Interactive menu:

```bash
python ollama_terminal.py
```

Run a task directly:

```bash
python ollama_terminal.py "find all .log files older than 7 days and delete them"
```

Choose a model manually:

```bash
python ollama_terminal.py -m mistral "check for flatpak updates"
```

System check:

```bash
python ollama_terminal.py --check
```

---

## âš™ Configuration

Inside `ollama_terminal.py`:

```python
OLLAMA_BASE      = "http://localhost:11434"
MAX_ITERATIONS   = 60
MAX_JSON_RETRIES = 5
MAX_HISTORY_MSGS = 16
```

You can adjust these if needed.

---

## ğŸ§  How It Works (Simple)

1. You describe a task.  
2. The model replies in strict JSON:
   ```json
   {"action":"run","command":"...","reason":"..."}
   ```
3. The script runs the command.  
4. The output is sent back to the model.  
5. The loop continues until:
   ```json
   {"action":"done","summary":"..."}
   ```

The model must verify success before finishing.

---

## ğŸ§© Recommended Models

- `llama3` â€” reliable all-rounder  
- `llama3.1` â€” strong reasoning  
- `mistral` â€” fast and stable  
- `qwen2.5-coder` â€” coding tasks  
- `deepseek-r1` â€” thorough reasoning  

âš  Very small models (<1B parameters) may struggle with strict JSON formatting.

---

## ğŸ›¡ Security Note

This tool executes **real shell commands**.

Be specific with tasks.  
Avoid vague destructive instructions such as:

```
clean my system
optimize everything
delete unused stuff
```

Always review what you are asking it to do.

---

## ğŸ“„ License

MIT
